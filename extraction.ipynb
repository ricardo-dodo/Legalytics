{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data manipulation\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "# For NLP\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "flat_data = json_normalize(data, record_path=['data']) # flatten the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>id</th>\n",
       "      <th>ref</th>\n",
       "      <th>type</th>\n",
       "      <th>bab</th>\n",
       "      <th>bagian</th>\n",
       "      <th>paragraf</th>\n",
       "      <th>pasal</th>\n",
       "      <th>level</th>\n",
       "      <th>context</th>\n",
       "      <th>additional_context</th>\n",
       "      <th>chunks</th>\n",
       "      <th>source_token_length</th>\n",
       "      <th>buku</th>\n",
       "      <th>alias</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BERITA NEGARA REPUBLIK INDONESIA No.920, 2017 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PERATURAN MENTERI KEUANGAN REPUBLIK INDONESIA ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DENGAN RAHMAT TUHAN YANG MAHA ESA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MENTERI KEUANGAN REPUBLIK INDONESIA,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>SRI MULYANI INDRAWATI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>Diundangkan di Jakarta pada tanggal 7 Juli 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2344</th>\n",
       "      <td>DIREKTUR JENDERAL PERATURAN PERUNDANG-UNDANGAN...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345</th>\n",
       "      <td>ttd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>WIDODO EKATJAHJANA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2347 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content   id  ref type  bab  \\\n",
       "0                                                        NaN  NaN  NaN  NaN   \n",
       "1     BERITA NEGARA REPUBLIK INDONESIA No.920, 2017 ...  NaN  NaN  NaN  NaN   \n",
       "2     PERATURAN MENTERI KEUANGAN REPUBLIK INDONESIA ...  NaN  NaN  NaN  NaN   \n",
       "3                     DENGAN RAHMAT TUHAN YANG MAHA ESA  NaN  NaN  NaN  NaN   \n",
       "4                  MENTERI KEUANGAN REPUBLIK INDONESIA,  NaN  NaN  NaN  NaN   \n",
       "...                                                 ...  ...  ...  ...  ...   \n",
       "2342                              SRI MULYANI INDRAWATI  NaN  NaN  NaN  NaN   \n",
       "2343    Diundangkan di Jakarta pada tanggal 7 Juli 2017  NaN  NaN  NaN  NaN   \n",
       "2344  DIREKTUR JENDERAL PERATURAN PERUNDANG-UNDANGAN...  NaN  NaN  NaN  NaN   \n",
       "2345                                                ttd  NaN  NaN  NaN  NaN   \n",
       "2346                                 WIDODO EKATJAHJANA  NaN  NaN  NaN  NaN   \n",
       "\n",
       "     bagian  paragraf pasal  level context additional_context chunks  \\\n",
       "0       NaN       NaN   NaN    NaN     NaN                NaN    NaN   \n",
       "1       NaN       NaN   NaN    NaN     NaN                NaN    NaN   \n",
       "2       NaN       NaN   NaN    NaN     NaN                NaN    NaN   \n",
       "3       NaN       NaN   NaN    NaN     NaN                NaN    NaN   \n",
       "4       NaN       NaN   NaN    NaN     NaN                NaN    NaN   \n",
       "...     ...       ...   ...    ...     ...                ...    ...   \n",
       "2342    NaN       NaN   NaN    NaN     NaN                NaN    NaN   \n",
       "2343    NaN       NaN   NaN    NaN     NaN                NaN    NaN   \n",
       "2344    NaN       NaN   NaN    NaN     NaN                NaN    NaN   \n",
       "2345    NaN       NaN   NaN    NaN     NaN                NaN    NaN   \n",
       "2346    NaN       NaN   NaN    NaN     NaN                NaN    NaN   \n",
       "\n",
       "      source_token_length  buku alias term  \n",
       "0                     NaN   NaN   NaN  NaN  \n",
       "1                     NaN   NaN   NaN  NaN  \n",
       "2                     NaN   NaN   NaN  NaN  \n",
       "3                     NaN   NaN   NaN  NaN  \n",
       "4                     NaN   NaN   NaN  NaN  \n",
       "...                   ...   ...   ...  ...  \n",
       "2342                  NaN   NaN   NaN  NaN  \n",
       "2343                  NaN   NaN   NaN  NaN  \n",
       "2344                  NaN   NaN   NaN  NaN  \n",
       "2345                  NaN   NaN   NaN  NaN  \n",
       "2346                  NaN   NaN   NaN  NaN  \n",
       "\n",
       "[2347 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2347 entries, 0 to 2346\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   content              2347 non-null   object \n",
      " 1   id                   1153 non-null   object \n",
      " 2   ref                  1201 non-null   object \n",
      " 3   type                 2044 non-null   object \n",
      " 4   bab                  1524 non-null   object \n",
      " 5   bagian               126 non-null    object \n",
      " 6   paragraf             0 non-null      float64\n",
      " 7   pasal                1533 non-null   object \n",
      " 8   level                1153 non-null   float64\n",
      " 9   context              801 non-null    object \n",
      " 10  additional_context   801 non-null    object \n",
      " 11  chunks               891 non-null    object \n",
      " 12  source_token_length  801 non-null    float64\n",
      " 13  buku                 0 non-null      float64\n",
      " 14  alias                90 non-null     object \n",
      " 15  term                 90 non-null     object \n",
      "dtypes: float64(4), object(12)\n",
      "memory usage: 293.5+ KB\n"
     ]
    }
   ],
   "source": [
    "flat_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3754f226b1ef4afdb1e6e60ec06b419b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Documents\\Github\\Parser_PUU\\.conda\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--indobenchmark--indobert-base-p1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Set up the IndoBERT NER model\n",
    "model_name = \"indobenchmark/indobert-base-p1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "At least one input is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 115\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m record\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Apply the process_record function to each row in the DataFrame\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m \u001b[43mflat_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Display the results or continue with more \u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(processed_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoney\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdates\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_terms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprohibitions\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Documents\\Github\\Parser_PUU\\.conda\\lib\\site-packages\\pandas\\core\\frame.py:10361\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10347\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10349\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10350\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10351\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10359\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10360\u001b[0m )\n\u001b[1;32m> 10361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Documents\\Github\\Parser_PUU\\.conda\\lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Documents\\Github\\Parser_PUU\\.conda\\lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Documents\\Github\\Parser_PUU\\.conda\\lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[8], line 115\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m record\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Apply the process_record function to each row in the DataFrame\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m flat_data\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mprocess_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Display the results or continue with more \u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(processed_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoney\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdates\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_terms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprohibitions\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "Cell \u001b[1;32mIn[8], line 109\u001b[0m, in \u001b[0;36mprocess_record\u001b[1;34m(record, tokenizer, model)\u001b[0m\n\u001b[0;32m    107\u001b[0m record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdates\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_dates(content)\n\u001b[0;32m    108\u001b[0m record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprohibitions\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_prohibitions(content)\n\u001b[1;32m--> 109\u001b[0m ner_results \u001b[38;5;241m=\u001b[39m \u001b[43mapply_ner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_terms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_key_terms(ner_results)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m record\n",
      "Cell \u001b[1;32mIn[8], line 73\u001b[0m, in \u001b[0;36mapply_ner\u001b[1;34m(text, tokenizer, model)\u001b[0m\n\u001b[0;32m     71\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenized_input)\n\u001b[0;32m     72\u001b[0m     ner_results \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mid2label[prediction] \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mtolist()]\n\u001b[1;32m---> 73\u001b[0m     ner_results \u001b[38;5;241m=\u001b[39m \u001b[43mner_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ner_results\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Documents\\Github\\Parser_PUU\\.conda\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:244\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    Classify each token of the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m          exists if the offsets are available within the tokenizer\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m     _inputs, offset_mapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_parser(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m offset_mapping:\n\u001b[0;32m    246\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m offset_mapping\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Documents\\Github\\Parser_PUU\\.conda\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:40\u001b[0m, in \u001b[0;36mTokenClassificationArgumentHandler.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one input is required.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m offset_mapping \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset_mapping:\n",
      "\u001b[1;31mValueError\u001b[0m: At least one input is required."
     ]
    }
   ],
   "source": [
    "def extract_money(text):\n",
    "    \"\"\"\n",
    "    Extracts money values from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text from which to extract money values.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of money values found in the text.\n",
    "    \"\"\"\n",
    "    money_pattern = re.compile(r'Rp\\s?\\d{1,3}(?:\\.\\d{3})*(?:,\\d{0,2})?\\s?')\n",
    "    return money_pattern.findall(text)\n",
    "\n",
    "def extract_dates(text):\n",
    "    \"\"\"\n",
    "    Extracts dates from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text from which dates need to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dates extracted from the text.\n",
    "    \"\"\"\n",
    "    date_pattern = re.compile(r'\\b(?:\\d{1,2}\\s(?:Januari|Februari|Maret|April|Mei|Juni|Juli|Agustus|September|Oktober|November|Desember)\\s\\d{4})\\b')\n",
    "    return date_pattern.findall(text)\n",
    "\n",
    "def extract_prohibitions(text):\n",
    "    \"\"\"\n",
    "    Extracts the list of prohibitions from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text from which to extract the prohibitions.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of prohibitions extracted from the text.\n",
    "    \"\"\"\n",
    "    prohibition_pattern = re.compile(r'\\b(dilarang)\\b:?\\s*(\\w\\.\\s*[^;]+;?|\\w\\.\\s*[^;]+dan)', re.IGNORECASE | re.MULTILINE)\n",
    "    prohibitions = prohibition_pattern.findall(text)\n",
    "    prohibitions_list = [item[1].strip() for item in prohibitions]\n",
    "    return prohibitions_list\n",
    "\n",
    "def extract_key_terms(entity_list):\n",
    "    \"\"\"Extracts key terms from a list of entities for summarization.\n",
    "\n",
    "    This function takes a list of entities and filters out the ones that belong to the 'TERM' or 'KEYWORD' entity group.\n",
    "    It returns a list of the words corresponding to these key terms.\n",
    "\n",
    "    Args:\n",
    "        entity_list (list): A list of dictionaries representing entities.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of words corresponding to the key terms.\n",
    "    \"\"\"\n",
    "    return [entity['word'] for entity in entity_list if entity['entity_group'] in ('TERM', 'KEYWORD')]\n",
    "\n",
    "def apply_ner(text, tokenizer, model):\n",
    "    \"\"\"Apply Named Entity Recognition (NER) on the given text.\n",
    "       Updated apply_ner function to work safely with transformers pipeline\n",
    "       \n",
    "    Args:\n",
    "        text (str): The input text to perform NER on.\n",
    "        tokenizer (Tokenizer): The tokenizer used to tokenize the text.\n",
    "        model (Model): The NER model used for prediction.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of predicted named entities in the text.\n",
    "    \"\"\"\n",
    "    ner_results = []\n",
    "    if text.strip():  # Ensure text is not empty\n",
    "        tokenized_input = tokenizer(text, truncation=True, max_length=512, padding='max_length', return_tensors=\"pt\")\n",
    "        output = model(**tokenized_input)\n",
    "        ner_results = [model.config.id2label[prediction] for prediction in output.logits.argmax(dim=-1).flatten().tolist()]\n",
    "        ner_results = ner_pipeline(tokenized_input)\n",
    "    return ner_results\n",
    "\n",
    "flat_data = flat_data.dropna(subset=['content'])\n",
    "\n",
    "def process_record(record, tokenizer, model):\n",
    "    \"\"\"Process a record by extracting relevant information.\n",
    "\n",
    "    Args:\n",
    "        record (dict): A dictionary representing a record with 'content' as one of the keys.\n",
    "        tokenizer: The tokenizer object used for tokenization.\n",
    "        model: The model object used for NER (Named Entity Recognition).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the processed record with the following keys:\n",
    "            - 'content': The original content of the record.\n",
    "            - 'money': A list of extracted money values.\n",
    "            - 'dates': A list of extracted dates.\n",
    "            - 'key_terms': A list of extracted key terms.\n",
    "            - 'prohibitions': A list of extracted prohibitions.\n",
    "    \"\"\"\n",
    "    content = record['content']\n",
    "\n",
    "    # Check if content is empty and handle it appropriately\n",
    "    if not content:\n",
    "        return {\n",
    "            'content': content,\n",
    "            'money': [],\n",
    "            'dates': [],\n",
    "            'key_terms': [],\n",
    "            'prohibitions': []\n",
    "        }\n",
    "\n",
    "    record['money'] = extract_money(content)\n",
    "    record['dates'] = extract_dates(content)\n",
    "    record['prohibitions'] = extract_prohibitions(content)\n",
    "    ner_results = apply_ner(content, tokenizer, model)\n",
    "    record['key_terms'] = extract_key_terms(ner_results)\n",
    "\n",
    "    return record\n",
    "\n",
    "# Apply the process_record function to each row in the DataFrame\n",
    "processed_data = flat_data.apply(lambda x: process_record(x, tokenizer, model), axis=1)\n",
    "\n",
    "# Display the results or continue with more \n",
    "print(processed_data[['content', 'money', 'dates', 'key_terms', 'prohibitions']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
