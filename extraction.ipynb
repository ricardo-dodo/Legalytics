{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ricardo/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# For data manipulation\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "# For NLP\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# For OpenSearch\n",
    "from opensearchpy import OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'additional_content' not found. If specifying a record_path, all elements of data should have the path.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/json/_normalize.py:398\u001b[0m, in \u001b[0;36mjson_normalize.<locals>._pull_field\u001b[0;34m(js, spec, extract_record)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'additional_content'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 35\u001b[0m\n\u001b[1;32m     29\u001b[0m chunks_data \u001b[38;5;241m=\u001b[39m json_normalize(data, record_path\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlocks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     30\u001b[0m                              meta\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeraturanId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNomor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSlug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJudul\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTahun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBentuk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBidang\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeraturanGoId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTanggalPenetapan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTanggalPengundangan\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     31\u001b[0m                                    [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlocks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlocks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlocks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlocks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlocks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbab\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlocks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbagian\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlocks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparagraf\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlocks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpasal\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlocks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_token_length\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[1;32m     32\u001b[0m                              record_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunks_\u001b[39m\u001b[38;5;124m'\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Normalize the nested data in 'additional_content'\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m additional_content_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlocks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madditional_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPeraturanId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNomor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSlug\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJudul\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTahun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBentuk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStatus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBidang\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPeraturanGoId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTanggalPenetapan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTanggalPengundangan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlocks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlocks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlocks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlocks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mref\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlocks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbab\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlocks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbagian\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlocks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparagraf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlocks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpasal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlocks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msource_token_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrecord_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madditional_content_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Normalize the top-level data\u001b[39;00m\n\u001b[1;32m     41\u001b[0m top_level_data \u001b[38;5;241m=\u001b[39m json_normalize(data)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/json/_normalize.py:517\u001b[0m, in \u001b[0;36mjson_normalize\u001b[0;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 meta_vals[key]\u001b[38;5;241m.\u001b[39mappend(meta_val)\n\u001b[1;32m    515\u001b[0m             records\u001b[38;5;241m.\u001b[39mextend(recs)\n\u001b[0;32m--> 517\u001b[0m \u001b[43m_recursive_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m result \u001b[38;5;241m=\u001b[39m DataFrame(records)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m record_prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/json/_normalize.py:496\u001b[0m, in \u001b[0;36mjson_normalize.<locals>._recursive_extract\u001b[0;34m(data, path, seen_meta, level)\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(val):\n\u001b[1;32m    494\u001b[0m                 seen_meta[key] \u001b[38;5;241m=\u001b[39m _pull_field(obj, val[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 496\u001b[0m         \u001b[43m_recursive_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/json/_normalize.py:499\u001b[0m, in \u001b[0;36mjson_normalize.<locals>._recursive_extract\u001b[0;34m(data, path, seen_meta, level)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m--> 499\u001b[0m         recs \u001b[38;5;241m=\u001b[39m \u001b[43m_pull_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m         recs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    501\u001b[0m             nested_to_record(r, sep\u001b[38;5;241m=\u001b[39msep, max_level\u001b[38;5;241m=\u001b[39mmax_level)\n\u001b[1;32m    502\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    503\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m r\n\u001b[1;32m    504\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m recs\n\u001b[1;32m    505\u001b[0m         ]\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;66;03m# For repeating the metadata later\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/json/_normalize.py:421\u001b[0m, in \u001b[0;36mjson_normalize.<locals>._pull_records\u001b[0;34m(js, spec)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pull_records\u001b[39m(js: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], spec: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    Internal function to pull field for records, and similar to\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m    _pull_field, but require to return list. And will raise error\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m    if has non iterable value.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_pull_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_record\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# GH 31507 GH 30145, GH 26284 if result is not list, raise TypeError if not\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;66;03m# null, otherwise return an empty list\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/json/_normalize.py:401\u001b[0m, in \u001b[0;36mjson_normalize.<locals>._pull_field\u001b[0;34m(js, spec, extract_record)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extract_record:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    402\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. If specifying a record_path, all elements of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata should have the path.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'additional_content' not found. If specifying a record_path, all elements of data should have the path.\""
     ]
    }
   ],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Set up OpenSearch connection with environment variables\n",
    "client = OpenSearch(\n",
    "    hosts=[{'host': ('62.72.7.91'), 'port': (('9200'))}],\n",
    "    http_auth=(('admin_jdih'), ('JDIHjuara6065')),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False\n",
    ")\n",
    "\n",
    "# Specify the document ID\n",
    "document_id = \"66-pmk.02-2013\"\n",
    "\n",
    "# Retrieve the document by ID\n",
    "response = client.get(index=\"law_analyzer_new4\", id=document_id)\n",
    "\n",
    "# Extract the document source\n",
    "data = response['_source']\n",
    "\n",
    "# Normalize the nested data in 'Blocks'\n",
    "blocks_data = json_normalize(data, record_path=['Blocks'], \n",
    "                             meta=['PeraturanId', 'Nomor', 'Slug', 'Judul', 'No', 'Tahun', 'Bentuk', 'Status', 'Bidang', 'Source', 'PeraturanGoId', 'TanggalPenetapan', 'TanggalPengundangan'],\n",
    "                             record_prefix='Blocks_', errors='ignore')\n",
    "\n",
    "# Normalize the nested data in 'chunks'\n",
    "chunks_data = json_normalize(data, record_path=['Blocks', 'chunks'],\n",
    "                             meta=['PeraturanId', 'Nomor', 'Slug', 'Judul', 'No', 'Tahun', 'Bentuk', 'Status', 'Bidang', 'Source', 'PeraturanGoId', 'TanggalPenetapan', 'TanggalPengundangan', \n",
    "                                   ['Blocks', 'content'], ['Blocks', 'context'], ['Blocks', 'type'], ['Blocks', 'ref'], ['Blocks', 'bab'], ['Blocks', 'bagian'], ['Blocks', 'paragraf'], ['Blocks', 'pasal'], ['Blocks', 'source_token_length']],\n",
    "                             record_prefix='chunks_', errors='ignore')\n",
    "\n",
    "# Normalize the nested data in 'additional_context'\n",
    "additional_context_data = json_normalize(data, record_path=['Blocks', 'additional_context'],\n",
    "                                         meta=['PeraturanId', 'Nomor', 'Slug', 'Judul', 'No', 'Tahun', 'Bentuk', 'Status', 'Bidang', 'Source', 'PeraturanGoId', 'TanggalPenetapan', 'TanggalPengundangan',\n",
    "                                               ['Blocks', 'content'], ['Blocks', 'context'], ['Blocks', 'type'], ['Blocks', 'ref'], ['Blocks', 'bab'], ['Blocks', 'bagian'], ['Blocks', 'paragraf'], ['Blocks', 'pasal'], ['Blocks', 'source_token_length']],\n",
    "                                         record_prefix='additional_context_', errors='ignore')\n",
    "\n",
    "# Normalize the top-level data\n",
    "top_level_data = json_normalize(data)\n",
    "\n",
    "# Merge the dataframes\n",
    "merged_data = top_level_data.merge(blocks_data, how='cross') \\\n",
    "                            .merge(chunks_data, how='outer') \\\n",
    "                            .merge(additional_context_data, how='outer')\n",
    "\n",
    "# Display the merged dataframe\n",
    "print(merged_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PeraturanId_x', 'Nomor_x', 'Slug_x', 'Judul_x', 'No_x', 'Tahun_x',\n",
       "       'Bentuk_x', 'Status_x', 'Bidang_x', 'Source_x', 'PeraturanGoId_x',\n",
       "       'TanggalPenetapan_x', 'TanggalPengundangan_x', 'Blocks', 'content',\n",
       "       'context', 'type', 'ref', 'bab', 'bagian', 'paragraf', 'pasal',\n",
       "       'source_token_length', 'chunks', 'additional_context', 'alias', 'term',\n",
       "       'PeraturanId_y', 'Nomor_y', 'Slug_y', 'Judul_y', 'No_y', 'Tahun_y',\n",
       "       'Bentuk_y', 'Status_y', 'Bidang_y', 'Source_y', 'PeraturanGoId_y',\n",
       "       'TanggalPenetapan_y', 'TanggalPengundangan_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list column names\n",
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [{'content': 'Dalam Peraturan Menteri ini, yan...\n",
       "Name: Blocks, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_data['Blocks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cahya/bert-base-indonesian-522M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Set up the IndoBERT NER model\n",
    "model_name = \"indobenchmark/indobert-base-p1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "ner_pipeline = pipeline(\"ner\", model=\"cahya/bert-base-indonesian-522M\", tokenizer=\"cahya/bert-base-indonesian-522M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prohibitions(text):\n",
    "    \"\"\"\n",
    "    Extracts the list of prohibitions from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text from which to extract the prohibitions.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of prohibitions extracted from the text.\n",
    "    \"\"\"\n",
    "    prohibition_pattern = re.compile(r'\\bDilarang:\\s*([^;]+)', re.IGNORECASE)\n",
    "    prohibitions = prohibition_pattern.findall(text)\n",
    "    return prohibitions\n",
    "\n",
    "def extract_dates(text):\n",
    "    \"\"\"\n",
    "    Extracts dates from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text from which dates need to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dates extracted from the text.\n",
    "    \"\"\"\n",
    "    date_pattern = re.compile(r'\\b(?:\\d{1,2}\\s(?:Januari|Februari|Maret|April|Mei|Juni|Juli|Agustus|September|Oktober|November|Desember)\\s\\d{4})\\b')\n",
    "    return date_pattern.findall(text)\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_money(text):\n",
    "    \"\"\"\n",
    "    Extracts money or monetary keywords from the given text, including multiple currencies.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text from which to extract money or monetary keywords.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of money or monetary keywords extracted from the text, including Rupiah, USD, and other major currencies.\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'RP': re.compile(r'\\b(?:\\(?Rp\\s*(?:\\d{1,3}(?:[,.]\\d{3})*(?:\\.\\d+)?|\\d+(?:\\.\\d+)?)\\)?|\\(?Rp\\s*(?:nol|nol,? nol)\\)?\\s*\\(?Rupiah\\)?)\\b', re.IGNORECASE),\n",
    "        'USD': re.compile(r\"(USD)([+-]?[0-9]{1,3}(,?[0-9]{3})*)(\\.[0-9]{1,4})\"),\n",
    "        'EUR': re.compile(r\"(€|EUR)([+-]?[0-9]{1,3}(,?[0-9]{3})*)(\\.[0-9]{1,4})\"),\n",
    "        'GBP': re.compile(r\"(£|GBP)([+-]?[0-9]{1,3}(,?[0-9]{3})*)(\\.[0-9]{1,4})\"),\n",
    "        'JPY': re.compile(r\"(¥|JPY)([+-]?[0-9]{1,3}(,?[0-9]{3})*)(\\.?[0-9]{0,4})\"),  # Yen typically doesn't use decimal places for common transactions\n",
    "    }\n",
    "\n",
    "    matches = []\n",
    "    for currency, pattern in patterns.items():\n",
    "        found_matches = pattern.findall(text)\n",
    "        for match in found_matches:\n",
    "            matches.append(''.join(match))\n",
    "\n",
    "    # Convert word numbers to digits for Rupiah\n",
    "    word_to_number = {\n",
    "        'satu': '1', 'dua': '2', 'tiga': '3', 'empat': '4', 'lima': '5',\n",
    "        'enam': '6', 'tujuh': '7', 'delapan': '8', 'sembilan': '9', 'nol': '0',\n",
    "        'ribu': '000', 'juta': '000000', 'miliar': '000000000', 'triliun': '000000000000'\n",
    "    }\n",
    "\n",
    "    numerical_matches = []\n",
    "    for match in matches:\n",
    "        for word, value in word_to_number.items():\n",
    "            if 'Rp' in match:  # Apply word-to-number conversion only for Rupiah\n",
    "                match = match.replace(word, value)\n",
    "        numerical_matches.append(match)\n",
    "\n",
    "    return numerical_matches\n",
    "\n",
    "\n",
    "def apply_ner(text, ner_pipeline, max_length=512):\n",
    "    \"\"\"Apply Named Entity Recognition (NER) using the transformers pipeline on the given text.\n",
    "    This version supports processing of text longer than 512 tokens by splitting the text into\n",
    "    manageable parts and then combining the results.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to perform NER on.\n",
    "        ner_pipeline (pipeline): The NER pipeline for prediction.\n",
    "        max_length (int): Maximum length of tokens to process in a single call to the NER pipeline.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of predicted named entities in the text.\n",
    "    \"\"\"\n",
    "    if not text.strip():  # Check if text is empty\n",
    "        return []\n",
    "\n",
    "    # Initialize variables\n",
    "    split_texts = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "    ner_results = []\n",
    "\n",
    "    # Process each split text\n",
    "    for split_text in split_texts:\n",
    "        results = ner_pipeline(split_text)\n",
    "        ner_results.extend(results)\n",
    "\n",
    "    return ner_results\n",
    "\n",
    "\n",
    "flat_data = flat_data.dropna(subset=['content'])\n",
    "\n",
    "def process_record(record, ner_pipeline):\n",
    "    \"\"\"Process a record by extracting relevant information.\n",
    "\n",
    "    Args:\n",
    "        record (pd.Series): A pandas Series representing a record with 'content' as one of the keys.\n",
    "        ner_pipeline: The NER pipeline object used for Named Entity Recognition.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the processed record with the following keys:\n",
    "            - 'content': The original content of the record.\n",
    "            - 'money': A list of extracted money values.\n",
    "            - 'dates': A list of extracted dates.\n",
    "            - 'prohibitions': A list of extracted prohibitions.\n",
    "            - 'named_entities': A list of named entities recognized by NER.\n",
    "    \"\"\"\n",
    "    content = record['content']\n",
    "\n",
    "    if not content:\n",
    "        return {\n",
    "            'content': content,\n",
    "            'money': [],\n",
    "            'dates': [],\n",
    "            'prohibitions': [],\n",
    "            'named_entities': []\n",
    "        }\n",
    "\n",
    "    result_dict = {\n",
    "        'content': content,\n",
    "        'money': extract_money(content),\n",
    "        'dates': extract_dates(content),\n",
    "        'prohibitions': extract_prohibitions(content),\n",
    "    }\n",
    "    ner_results = apply_ner(content, ner_pipeline)\n",
    "\n",
    "    return result_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                content money          dates  \\\n",
      "0                                                          []             []   \n",
      "1     BERITA NEGARA REPUBLIK INDONESIA No.920, 2017 ...    []             []   \n",
      "2     PERATURAN MENTERI KEUANGAN REPUBLIK INDONESIA ...    []             []   \n",
      "3                     DENGAN RAHMAT TUHAN YANG MAHA ESA    []             []   \n",
      "4                  MENTERI KEUANGAN REPUBLIK INDONESIA,    []             []   \n",
      "...                                                 ...   ...            ...   \n",
      "2342                              SRI MULYANI INDRAWATI    []             []   \n",
      "2343    Diundangkan di Jakarta pada tanggal 7 Juli 2017    []  [7 Juli 2017]   \n",
      "2344  DIREKTUR JENDERAL PERATURAN PERUNDANG-UNDANGAN...    []             []   \n",
      "2345                                                ttd    []             []   \n",
      "2346                                 WIDODO EKATJAHJANA    []             []   \n",
      "\n",
      "     prohibitions named_entities  \n",
      "0              []            NaN  \n",
      "1              []            NaN  \n",
      "2              []            NaN  \n",
      "3              []            NaN  \n",
      "4              []            NaN  \n",
      "...           ...            ...  \n",
      "2342           []            NaN  \n",
      "2343           []            NaN  \n",
      "2344           []            NaN  \n",
      "2345           []            NaN  \n",
      "2346           []            NaN  \n",
      "\n",
      "[2347 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ensure you have a DataFrame 'flat_data' with a 'content' column available.\n",
    "# Create a new DataFrame to store the processed records\n",
    "processed_records = [process_record(row, ner_pipeline) for index, row in flat_data.iterrows()]\n",
    "processed_df = pd.DataFrame(processed_records)\n",
    "\n",
    "# Now `processed_df` contains the original content and the extracted information.\n",
    "print(processed_df)  # To inspect the first few records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop named_entities column\n",
    "# processed_df = processed_df.drop(columns='named_entities')\n",
    "for col in processed_df.columns:\n",
    "    processed_df[col] = processed_df[col].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in money column:\n",
      "['' 'Rp0, Rp0']\n",
      "\n",
      "Unique values in dates column:\n",
      "['' '8 Juni 2017' '5 Juli 2017' '7 Juli 2017' '16 Januari 2023'\n",
      " '17 Januari 2023' '23 April 2019' '18 Oktober 2019']\n",
      "\n",
      "Unique values in prohibitions column:\n",
      "[''\n",
      " 'a. menggunakan kewenangan yang dimiliki untuk kepentingan pribadi, keluarga, dan/atau golongan']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#unique column money, dates, prohibitions and print all of them with for loop\n",
    "for col in ['money', 'dates', 'prohibitions']:\n",
    "    print(f'Unique values in {col} column:')\n",
    "    print(processed_df[col].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_csv('processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data manipulation\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "# For NLP\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# For OpenSearch\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "# Set up OpenSearch connection with environment variables\n",
    "client = OpenSearch(\n",
    "    hosts=[{'host': ('62.72.7.91'), 'port': (('9200'))}],\n",
    "    http_auth=(('admin_jdih'), ('JDIHjuara6065')),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False\n",
    ")\n",
    "\n",
    "client.info()\n",
    "\n",
    "# Specify the document ID\n",
    "document_id = \"66-pmk.02-2013\"\n",
    "\n",
    "# Retrieve the document by ID\n",
    "response = client.get(index=\"law_analyzer_new4\", id=document_id)\n",
    "\n",
    "# Extract the document source\n",
    "data = response['_source']\n",
    "\n",
    "flat_data = json_normalize(data)  # flatten the data\n",
    "\n",
    "# Separate the 'Blocks' column into new columns\n",
    "blocks_data = json_normalize(flat_data['Blocks'].explode())\n",
    "flat_data = pd.concat([flat_data, blocks_data], axis=1)\n",
    "flat_data = flat_data.drop(columns=['Blocks'])\n",
    "\n",
    "flat_data.info()\n",
    "\n",
    "# Set up the IndoBERT NER model\n",
    "model_name = \"indobenchmark/indobert-base-p1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "ner_pipeline = pipeline(\"ner\", model=\"cahya/bert-base-indonesian-522M\", tokenizer=\"cahya/bert-base-indonesian-522M\")\n",
    "\n",
    "def extract_prohibitions(text):\n",
    "    \"\"\"\n",
    "    Extracts the list of prohibitions from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text from which to extract the prohibitions.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of prohibitions extracted from the text.\n",
    "    \"\"\"\n",
    "    prohibition_pattern = re.compile(r'(?:dilarang|Dilarang):\\s*([a-z](?:(?!(?:dilarang|Dilarang):).)*)\\.', re.IGNORECASE | re.DOTALL)\n",
    "    prohibitions = prohibition_pattern.findall(text)\n",
    "\n",
    "    cleaned_prohibitions = []\n",
    "    for prohibition in prohibitions:\n",
    "        cleaned_prohibition = re.sub(r'\\n', ' ', prohibition).strip()\n",
    "        cleaned_prohibitions.append(cleaned_prohibition)\n",
    "\n",
    "    return cleaned_prohibitions\n",
    "\n",
    "def extract_dates(text):\n",
    "    \"\"\"\n",
    "    Extracts dates from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text from which dates need to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dates extracted from the text.\n",
    "    \"\"\"\n",
    "    date_pattern = re.compile(r'\\b(?:\\d{1,2}\\s(?:Januari|Februari|Maret|April|Mei|Juni|Juli|Agustus|September|Oktober|November|Desember)\\s\\d{4})\\b')\n",
    "    return date_pattern.findall(text)\n",
    "\n",
    "def extract_money(text):\n",
    "    \"\"\"\n",
    "    Extracts money or monetary keywords from the given text, including multiple currencies.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text from which to extract money or monetary keywords.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of money or monetary keywords extracted from the text, including Rupiah, USD, and other major currencies.\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'RP': re.compile(r'\\b(?:\\(?Rp\\s*(?:\\d{1,3}(?:[,.]\\d{3})*(?:\\.\\d+)?|\\d+(?:\\.\\d+)?)\\)?|\\(?Rp\\s*(?:nol|nol,? nol)\\)?\\s*\\(?Rupiah\\)?)\\b', re.IGNORECASE),\n",
    "        'USD': re.compile(r\"(USD)([+-]?[0-9]{1,3}(,?[0-9]{3})*)(\\.[0-9]{1,4})\"),\n",
    "        'EUR': re.compile(r\"(€|EUR)([+-]?[0-9]{1,3}(,?[0-9]{3})*)(\\.[0-9]{1,4})\"),\n",
    "        'GBP': re.compile(r\"(£|GBP)([+-]?[0-9]{1,3}(,?[0-9]{3})*)(\\.[0-9]{1,4})\"),\n",
    "        'JPY': re.compile(r\"(¥|JPY)([+-]?[0-9]{1,3}(,?[0-9]{3})*)(\\.?[0-9]{0,4})\"),  # Yen typically doesn't use decimal places for common transactions\n",
    "    }\n",
    "\n",
    "    matches = []\n",
    "    for currency, pattern in patterns.items():\n",
    "        found_matches = pattern.findall(text)\n",
    "        for match in found_matches:\n",
    "            matches.append(''.join(match))\n",
    "\n",
    "    # Convert word numbers to digits for Rupiah\n",
    "    word_to_number = {\n",
    "        'satu': '1', 'dua': '2', 'tiga': '3', 'empat': '4', 'lima': '5',\n",
    "        'enam': '6', 'tujuh': '7', 'delapan': '8', 'sembilan': '9', 'nol': '0',\n",
    "        'ribu': '000', 'juta': '000000', 'miliar': '000000000', 'triliun': '000000000000'\n",
    "    }\n",
    "\n",
    "    numerical_matches = []\n",
    "    for match in matches:\n",
    "        for word, value in word_to_number.items():\n",
    "            if 'Rp' in match:  # Apply word-to-number conversion only for Rupiah\n",
    "                match = match.replace(word, value)\n",
    "        numerical_matches.append(match)\n",
    "\n",
    "    return numerical_matches\n",
    "\n",
    "\n",
    "def apply_ner(text, ner_pipeline, max_length=512):\n",
    "    \"\"\"Apply Named Entity Recognition (NER) using the transformers pipeline on the given text.\n",
    "    This version supports processing of text longer than 512 tokens by splitting the text into\n",
    "    manageable parts and then combining the results.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to perform NER on.\n",
    "        ner_pipeline (pipeline): The NER pipeline for prediction.\n",
    "        max_length (int): Maximum length of tokens to process in a single call to the NER pipeline.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of predicted named entities in the text.\n",
    "    \"\"\"\n",
    "    if not text.strip():  # Check if text is empty\n",
    "        return []\n",
    "\n",
    "    # Initialize variables\n",
    "    split_texts = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "    ner_results = []\n",
    "\n",
    "    # Process each split text\n",
    "    for split_text in split_texts:\n",
    "        results = ner_pipeline(split_text)\n",
    "        ner_results.extend(results)\n",
    "\n",
    "    return ner_results\n",
    "\n",
    "\n",
    "flat_data = flat_data.dropna(subset=['content'])\n",
    "\n",
    "def process_record(record, ner_pipeline):\n",
    "    \"\"\"Process a record by extracting relevant information.\n",
    "\n",
    "    Args:\n",
    "        record (pd.Series): A pandas Series representing a record with 'content' as one of the keys.\n",
    "        ner_pipeline: The NER pipeline object used for Named Entity Recognition.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the processed record with the following keys:\n",
    "            - 'content': The original content of the record.\n",
    "            - 'money': A list of extracted money values.\n",
    "            - 'dates': A list of extracted dates.\n",
    "            - 'prohibitions': A list of extracted prohibitions.\n",
    "            - 'named_entities': A list of named entities recognized by NER.\n",
    "    \"\"\"\n",
    "    content = record['content']\n",
    "\n",
    "    if not content:\n",
    "        return {\n",
    "            'content': content,\n",
    "            'money': [],\n",
    "            'dates': [],\n",
    "            'prohibitions': [],\n",
    "            'named_entities': []\n",
    "        }\n",
    "\n",
    "    result_dict = {\n",
    "        'content': content,\n",
    "        'money': extract_money(content),\n",
    "        'dates': extract_dates(content),\n",
    "        'prohibitions': extract_prohibitions(content),\n",
    "    }\n",
    "    ner_results = apply_ner(content, ner_pipeline)\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "# Ensure you have a DataFrame 'flat_data' with a 'content' column available.\n",
    "# Create a new DataFrame to store the processed records\n",
    "processed_records = [process_record(row, ner_pipeline) for index, row in flat_data.iterrows()]\n",
    "processed_df = pd.DataFrame(processed_records)\n",
    "\n",
    "# Now `processed_df` contains the original content and the extracted information.\n",
    "print(processed_df)  # To inspect the first few records.\n",
    "\n",
    "for col in processed_df.columns:\n",
    "    processed_df[col] = processed_df[col].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Unique values in money, dates, and prohibitions columns\n",
    "for col in ['money', 'dates', 'prohibitions']:\n",
    "    print(f'Unique values in {col} column:')\n",
    "    print(processed_df[col].unique())\n",
    "    print()\n",
    "\n",
    "processed_df.to_csv('processed_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
